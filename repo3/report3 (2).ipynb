{"metadata":{"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys, os\nsys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\nfrom common.functions import *\nfrom common.gradient import numerical_gradient\n\nclass TwoLayerNet:\n\n    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n        self.params = {}\n        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n        self.params['b1'] = np.zeros(hidden_size)\n        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n        self.params['b2'] = np.zeros(output_size)\n\n    def predict(self, x):\n        W1, W2 = self.params['W1'], self.params['W2']\n        b1, b2 = self.params['b1'], self.params['b2']\n    \n        a1 = np.dot(x, W1) + b1\n        z1 = sigmoid(a1)\n        a2 = np.dot(z1, W2) + b2\n        y = softmax(a2)\n       \n        return y\n       \n    # x:入力データ, t:教師データ\n    def loss(self, x, t):\n        y = self.predict(x)\n       \n        return cross_entropy_error(y, t)\n    \n    def accuracy(self, x, t):\n        y = self.predict(x)\n        y = np.argmax(y, axis=1)\n        t = np.argmax(t, axis=1)\n        \n        accuracy = np.sum(y == t) / float(x.shape[0])\n        return accuracy\n        \n    # x:入力データ, t:教師データ\n    def numerical_gradient(self, x, t):\n        loss_W = lambda W: self.loss(x, t)\n        \n        grads = {}\n        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n        \n        return grads\n        \n    def gradient(self, x, t):\n        W1, W2 = self.params['W1'], self.params['W2']\n        b1, b2 = self.params['b1'], self.params['b2']\n        grads = {}\n        \n        batch_num = x.shape[0]\n        \n        # forward\n        a1 = np.dot(x, W1) + b1\n        z1 = sigmoid(a1)\n        a2 = np.dot(z1, W2) + b2\n        y = softmax(a2)\n        \n        # backward\n        dy = (y - t) / batch_num\n        grads['W2'] = np.dot(z1.T, dy)\n        grads['b2'] = np.sum(dy, axis=0)\n        \n        da1 = np.dot(dy, W2.T)\n        dz1 = sigmoid_grad(a1) * da1\n        grads['W1'] = np.dot(x.T, dz1)\n        grads['b1'] = np.sum(dz1, axis=0)\n\n        return grads\n    ","metadata":{"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"net.params['W1'].shape","metadata":{"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"(784, 100)"},"metadata":{}}]},{"cell_type":"code","source":"net.params['W2'].shape","metadata":{"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(100, 10)"},"metadata":{}}]},{"cell_type":"code","source":"net.params['b1'].shape","metadata":{"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(100,)"},"metadata":{}}]},{"cell_type":"code","source":"net.params['b2'].shape","metadata":{"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(10,)"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom dataset.mnist import load_mnist\n\n# データの読み込み\n(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True) \n\nnetwork = TwoLayerNet(input_size=784, hidden_size=50, output_size=10) \n\niters_num = 10000  # 繰り返しの回数を適宜設定する\ntrain_size = x_train.shape[0]\nbatch_size = 100\nlearning_rate = 0.1\n\ntrain_loss_list = []\ntrain_acc_list = []\ntest_acc_list = []\n\niter_per_epoch = max(train_size / batch_size, 1) \n\nfor i in range(iters_num):\n    batch_mask = np.random.choice(train_size, batch_size)\n    x_batch = x_train[batch_mask]\n    t_batch = t_train[batch_mask]\n    \n    # 勾配の計算\n    #grad = network.numerical_gradient(x_batch, t_batch)\n    grad = network.gradient(x_batch, t_batch)\n    \n    # パラメータの更新\n    for key in ('W1', 'b1', 'W2', 'b2'):\n        network.params[key] -= learning_rate * grad[key]\n    \n    loss = network.loss(x_batch, t_batch)\n    train_loss_list.append(loss)\n    \n    if i % iter_per_epoch == 0:\n        train_acc = network.accuracy(x_train, t_train)\n        test_acc = network.accuracy(x_test, t_test)\n        train_acc_list.append(train_acc)\n        test_acc_list.append(test_acc)\n        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n\n# グラフの描画\nmarkers = {'train': 'o', 'test': 's'}\nx = np.arange(len(train_acc_list))\nplt.plot(x, train_acc_list, label='train acc')\nplt.plot(x, test_acc_list, label='test acc', linestyle='--')\nplt.xlabel(\"epochs\")\nplt.ylabel(\"accuracy\")\nplt.ylim(0, 1.0)\nplt.legend(loc='lower right')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ここでプログラムの実行が止まってしまった。\nこのプログラムはブラウザ上でjupyternotebookを動かしているので、うまく環境構築できないまま実行しているからだと考えられる。また、自分のニュートラルネットワークについての理解も追いついていないことも原因の一つだと考えられるので、次の講義までにその知識をしっかり習得しておくことにする。","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}